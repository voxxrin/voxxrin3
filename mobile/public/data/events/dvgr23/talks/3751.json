{
  "speakers": [
    {
      "fullName": "Stavros Kontopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/AEdFTp5khjkC8f52xO77V8WgVDExcZV6WyT1ZERK0xrsrQ=s96-c",
      "id": "3611",
      "companyName": "Red Hat"
    }
  ],
  "format": {
    "duration": "PT40m",
    "title": "Conference",
    "id": "953"
  },
  "title": "Serverless Machine Learning Model Inference on Kubernetes with KServe",
  "language": "EN",
  "track": {
    "title": "Data & AI",
    "id": "1252"
  },
  "description": "<p><strong style=\"background-color: transparent; color: rgb(74, 134, 232);\">As a Machine Learning (ML) practitioner did you ever wish there was a Kubernetes native way of serving and scaling an ML model? Fortunately, there is Kserve!&nbsp;</strong></p><p><strong style=\"background-color: transparent; color: rgb(74, 134, 232);\">In this talk we will go through:</strong></p><ul><li><strong style=\"background-color: transparent;\">How to integrate with the most popular ML frameworks to allow easy model inference and prototyping.</strong></li><li><strong style=\"background-color: transparent;\">How to work with Knative Serving, a sophisticated extension of Kubernetes standard services, for cost-effective autoscaling of your ML models.</strong></li><li><strong style=\"background-color: transparent;\">How to get your deployments to the next level using the inference graph to build complex ML pipelines.</strong></li><li><strong style=\"background-color: transparent;\">How to monitor your models and effectively deploy them in production with different rollout strategies.&nbsp;</strong></li></ul><p><strong style=\"background-color: transparent; color: rgb(74, 134, 232);\">After this talk you</strong><strong style=\"color: rgb(74, 134, 232);\"> will be able to use the above technology in practice to deploy your own models as different scenarios will be described in detail.</strong></p>",
  "summary": "",
  "id": "3751",
  "room": {
    "title": "MC3",
    "id": "2803"
  }
}