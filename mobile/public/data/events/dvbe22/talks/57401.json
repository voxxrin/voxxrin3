{
  "title": "Word Embeddings from Deep Space Nine using Machine Learning.",
  "speakers": [
    {
      "fullName": "Gretel De Paepe",
      "companyName": "Collibra",
      "id": "53454",
      "photoUrl": "https://devoxxian-image-thumbnails.s3-eu-west-1.amazonaws.com/profile-5fac9d18-f409-4990-bc43-225e0299f2ba.png"
    },
    {
      "fullName": "Nick Evers",
      "id": "53455",
      "companyName": "Collibra",
      "photoUrl": "https://devoxxian-image-thumbnails.s3-eu-west-1.amazonaws.com/profile-dd08815d-1518-4a97-91b6-015b125087f5.jpg"
    }
  ],
  "id": "57401",
  "summary": "",
  "format": {
    "duration": "PT50m",
    "id": "951",
    "title": "Conference"
  },
  "room": {
    "id": "4705",
    "title": "Room 3"
  },
  "language": "EN",
  "description": "<p>Sisko - Man + Woman = ?</p><p>Word embeddings are a way to represent words in a numerical manner so they can be fed into machine learning models. In this presentation we will walk through some of the most common strategies, such as Bag of Words, Word2Vec, fastText, GloVe and Bert. Using the scripts of Star Trek Deep Space Nine, we will explore the differences between these techniques and how to use them in a production environment.</p><p><br></p>",
  "track": {
    "id": "1252",
    "title": "Data & AI"
  }
}